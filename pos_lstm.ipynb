{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26193588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, LSTM\n",
    "from transformers import DataCollatorForTokenClassification, RobertaPreTrainedModel, RobertaModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0160c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"vua_dataset\"\n",
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188d5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK punkt tokenizer data if you haven't already\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' tokenizer data not found. Downloading...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"NLTK 'punkt' tokenizer data downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during NLTK data check/download: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c419cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name is something like \"roberta-base\"\n",
    "if \"roberta\" in model_name.lower():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,  # required for pre-tokenized input with RoBERTa\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b9fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data_with_all_features(json_path):\n",
    "    \"\"\"\n",
    "    Loads raw data from a JSONL file, groups it by sentence,\n",
    "    and processes it to include both POS and FGPOS tags.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): The path to the JSONL data file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing \"sentence_words\", \"labels\", \n",
    "              \"pos_tags\", and \"fgpos_tags\".\n",
    "        set: A set of all unique POS tags.\n",
    "        set: A set of all unique FGPOS tags.\n",
    "    \"\"\"\n",
    "    data_raw = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data_raw.append(json.loads(line))\n",
    "\n",
    "    sentence_groups = defaultdict(list)\n",
    "    for entry in data_raw:\n",
    "        sentence_groups[entry[\"sentence\"]].append(entry)\n",
    "\n",
    "    processed_data = []\n",
    "    all_pos_tags = set()\n",
    "    all_fgpos_tags = set()\n",
    "    for sentence, entries in sentence_groups.items():\n",
    "        entries = sorted(entries, key=lambda x: x[\"w_index\"])\n",
    "        \n",
    "        original_words = sentence.split(' ')\n",
    "        words_for_model = [original_words[e['w_index']] for e in entries]\n",
    "        \n",
    "        current_labels = [entry[\"label\"] for entry in entries]\n",
    "        pos_tags_for_sentence = [entry[\"POS\"] for entry in entries]\n",
    "        fgpos_tags_for_sentence = [entry[\"FGPOS\"] for entry in entries]\n",
    "        \n",
    "        all_pos_tags.update(pos_tags_for_sentence)\n",
    "        all_fgpos_tags.update(fgpos_tags_for_sentence)\n",
    "\n",
    "        processed_data.append({\n",
    "            \"sentence_words\": words_for_model, \n",
    "            \"labels\": current_labels,\n",
    "            \"pos_tags\": pos_tags_for_sentence,\n",
    "            \"fgpos_tags\": fgpos_tags_for_sentence\n",
    "        })\n",
    "\n",
    "    return processed_data, all_pos_tags, all_fgpos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee189eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS vocabulary size: 17\n",
      "FGPOS vocabulary size: 41\n",
      "Number of training samples: 10909\n",
      "Number of test samples: 3601\n"
     ]
    }
   ],
   "source": [
    "# --- Load and process TRAIN data ---\n",
    "train_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_train.json\")\n",
    "processed_train_data, train_pos_tags, train_fgpos_tags = load_and_process_data_with_all_features(train_json_path)\n",
    "\n",
    "# --- Load and process TEST data ---\n",
    "test_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_test.json\")\n",
    "processed_test_data, test_pos_tags, test_fgpos_tags = load_and_process_data_with_all_features(test_json_path)\n",
    "\n",
    "# --- Create POS tag vocabulary ---\n",
    "all_pos_tags = sorted(list(train_pos_tags.union(test_pos_tags)))\n",
    "pos2id = {tag: i for i, tag in enumerate(all_pos_tags)}\n",
    "pos_vocab_size = len(pos2id)\n",
    "\n",
    "# --- Create FGPOS tag vocabulary ---\n",
    "all_fgpos_tags = sorted(list(train_fgpos_tags.union(test_fgpos_tags)))\n",
    "fgpos2id = {tag: i for i, tag in enumerate(all_fgpos_tags)}\n",
    "fgpos_vocab_size = len(fgpos2id)\n",
    "\n",
    "print(f\"POS vocabulary size: {pos_vocab_size}\")\n",
    "print(f\"FGPOS vocabulary size: {fgpos_vocab_size}\")\n",
    "print(f\"Number of training samples: {len(processed_train_data)}\")\n",
    "print(f\"Number of test samples: {len(processed_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0e8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDatasetWithAllFeatures(Dataset):\n",
    "    def __init__(self, data, pos2id, fgpos2id):\n",
    "        self.data = data\n",
    "        self.pos2id = pos2id\n",
    "        self.fgpos2id = fgpos2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        sentence_words = entry[\"sentence_words\"]\n",
    "        word_labels = entry[\"labels\"]\n",
    "        word_pos_tags = entry[\"pos_tags\"]\n",
    "        word_fgpos_tags = entry[\"fgpos_tags\"]\n",
    "\n",
    "        raw_encoding = tokenizer(\n",
    "            sentence_words,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "\n",
    "        word_ids = raw_encoding.word_ids(batch_index=0)\n",
    "\n",
    "        labels = []\n",
    "        pos_ids = []\n",
    "        fgpos_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "                pos_ids.append(-100)\n",
    "                fgpos_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(word_labels[word_idx])\n",
    "                pos_ids.append(self.pos2id[word_pos_tags[word_idx]])\n",
    "                fgpos_ids.append(self.fgpos2id[word_fgpos_tags[word_idx]])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "                pos_ids.append(-100)\n",
    "                fgpos_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        encoding = {k: torch.tensor(v).squeeze(0) for k, v in raw_encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "        encoding[\"pos_tag_ids\"] = torch.tensor(pos_ids, dtype=torch.long)\n",
    "        encoding[\"fgpos_tag_ids\"] = torch.tensor(fgpos_ids, dtype=torch.long)\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18d607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MetaphorDatasetWithAllFeatures(processed_train_data, pos2id, fgpos2id)\n",
    "test_dataset = MetaphorDatasetWithAllFeatures(processed_test_data, pos2id, fgpos2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ce1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaForTokenClassificationWithLSTM(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, pos_vocab_size, fgpos_vocab_size, pos_embedding_dim=50, fgpos_embedding_dim=50, lstm_hidden_size=128):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        self.pos_embedding = torch.nn.Embedding(pos_vocab_size, pos_embedding_dim)\n",
    "        self.fgpos_embedding = torch.nn.Embedding(fgpos_vocab_size, fgpos_embedding_dim)\n",
    "        \n",
    "        # The input to the LSTM is the concatenation of RoBERTa's output and the feature embeddings\n",
    "        lstm_input_size = config.hidden_size + pos_embedding_dim + fgpos_embedding_dim\n",
    "        \n",
    "        self.lstm = LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=1, # A single layer is often sufficient on top of RoBERTa\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        # The classifier input is the output of the Bi-LSTM (hidden_size * 2 for bidirectional)\n",
    "        self.classifier = torch.nn.Linear(lstm_hidden_size * 2, config.num_labels)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        pos_tag_ids=None,\n",
    "        fgpos_tag_ids=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs,\n",
    "        )\n",
    "        sequence_output = roberta_output[0]\n",
    "\n",
    "        # --- Get POS embeddings ---\n",
    "        pos_mask = pos_tag_ids != -100\n",
    "        cloned_pos_tag_ids = pos_tag_ids.clone()\n",
    "        cloned_pos_tag_ids[~pos_mask] = 0\n",
    "        pos_embeddings = self.pos_embedding(cloned_pos_tag_ids)\n",
    "        pos_embeddings[~pos_mask] = torch.zeros_like(pos_embeddings[~pos_mask])\n",
    "\n",
    "        # --- Get FGPOS embeddings ---\n",
    "        fgpos_mask = fgpos_tag_ids != -100\n",
    "        cloned_fgpos_tag_ids = fgpos_tag_ids.clone()\n",
    "        cloned_fgpos_tag_ids[~fgpos_mask] = 0\n",
    "        fgpos_embeddings = self.fgpos_embedding(cloned_fgpos_tag_ids)\n",
    "        fgpos_embeddings[~fgpos_mask] = torch.zeros_like(fgpos_embeddings[~fgpos_mask])\n",
    "\n",
    "        # --- Combine embeddings ---\n",
    "        combined_output = torch.cat([sequence_output, pos_embeddings, fgpos_embeddings], dim=-1)\n",
    "        \n",
    "        # --- Pass through Bi-LSTM ---\n",
    "        lstm_output, _ = self.lstm(combined_output)\n",
    "        \n",
    "        # --- Final Classification ---\n",
    "        dropped_output = self.dropout(lstm_output)\n",
    "        logits = self.classifier(dropped_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=roberta_output.hidden_states,\n",
    "            attentions=roberta_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae051c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define compute_metrics function ---\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for p_val, l_val in zip(prediction, label):\n",
    "            if l_val != -100:\n",
    "                true_labels.append(l_val)\n",
    "                predicted_labels.append(p_val)\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predicted_labels, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ae80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(train_dataset):\n",
    "    labels_list = [x['labels'].numpy() for x in train_dataset]\n",
    "    labels_flat = np.concatenate(labels_list)\n",
    "    labels_filtered = labels_flat[labels_flat != -100]\n",
    "    counts = Counter(labels_filtered)\n",
    "    \n",
    "    if len(counts) < 2:\n",
    "        return torch.tensor([1.0, 1.0], dtype=torch.float), counts, 0\n",
    "\n",
    "    total = sum(counts.values())\n",
    "    weight_0 = total / counts.get(0, 1)\n",
    "    weight_1 = total / counts.get(1, 1)\n",
    "    \n",
    "    return torch.tensor(\n",
    "        [weight_0, weight_1], dtype=torch.float\n",
    "    ), counts, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bbb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            logits = outputs.logits\n",
    "            labels = inputs.get(\"labels\")\n",
    "            \n",
    "            active_loss = labels.view(-1) != -100\n",
    "            active_logits = logits.view(-1, model.config.num_labels)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "\n",
    "            weights = self.class_weights.to(logits.device)\n",
    "            loss_fct = CrossEntropyLoss(weight=weights)\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "            \n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "        return outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65fb68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # number of folds\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "fold_f1s = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb21145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassificationWithLSTM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'fgpos_embedding.weight', 'lstm.bias_hh_l0', 'lstm.bias_hh_l0_reverse', 'lstm.bias_ih_l0', 'lstm.bias_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_hh_l0_reverse', 'lstm.weight_ih_l0', 'lstm.weight_ih_l0_reverse', 'pos_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1638' max='1638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1638/1638 07:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.278145</td>\n",
       "      <td>0.858704</td>\n",
       "      <td>0.612979</td>\n",
       "      <td>0.459194</td>\n",
       "      <td>0.921638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.235512</td>\n",
       "      <td>0.907838</td>\n",
       "      <td>0.699730</td>\n",
       "      <td>0.578825</td>\n",
       "      <td>0.884479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.253156</td>\n",
       "      <td>0.922140</td>\n",
       "      <td>0.726940</td>\n",
       "      <td>0.632990</td>\n",
       "      <td>0.853640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for fold 1 to results_with_lstm\\fold_1\n",
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassificationWithLSTM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'fgpos_embedding.weight', 'lstm.bias_hh_l0', 'lstm.bias_hh_l0_reverse', 'lstm.bias_ih_l0', 'lstm.bias_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_hh_l0_reverse', 'lstm.weight_ih_l0', 'lstm.weight_ih_l0_reverse', 'pos_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1638' max='1638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1638/1638 09:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.259239</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.680970</td>\n",
       "      <td>0.559711</td>\n",
       "      <td>0.869299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>0.917683</td>\n",
       "      <td>0.719447</td>\n",
       "      <td>0.610775</td>\n",
       "      <td>0.875159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.232630</td>\n",
       "      <td>0.921002</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.623001</td>\n",
       "      <td>0.873631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for fold 2 to results_with_lstm\\fold_2\n",
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassificationWithLSTM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'fgpos_embedding.weight', 'lstm.bias_hh_l0', 'lstm.bias_hh_l0_reverse', 'lstm.bias_ih_l0', 'lstm.bias_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_hh_l0_reverse', 'lstm.weight_ih_l0', 'lstm.weight_ih_l0_reverse', 'pos_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1638' max='1638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1638/1638 09:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.272702</td>\n",
       "      <td>0.894925</td>\n",
       "      <td>0.670167</td>\n",
       "      <td>0.541612</td>\n",
       "      <td>0.878740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.255982</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.702202</td>\n",
       "      <td>0.586980</td>\n",
       "      <td>0.873709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.266278</td>\n",
       "      <td>0.918661</td>\n",
       "      <td>0.720583</td>\n",
       "      <td>0.618316</td>\n",
       "      <td>0.863384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for fold 3 to results_with_lstm\\fold_3\n",
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassificationWithLSTM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'fgpos_embedding.weight', 'lstm.bias_hh_l0', 'lstm.bias_hh_l0_reverse', 'lstm.bias_ih_l0', 'lstm.bias_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_hh_l0_reverse', 'lstm.weight_ih_l0', 'lstm.weight_ih_l0_reverse', 'pos_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1638' max='1638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1638/1638 09:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.314900</td>\n",
       "      <td>0.268090</td>\n",
       "      <td>0.897267</td>\n",
       "      <td>0.661495</td>\n",
       "      <td>0.535109</td>\n",
       "      <td>0.866044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.237100</td>\n",
       "      <td>0.233916</td>\n",
       "      <td>0.911998</td>\n",
       "      <td>0.702068</td>\n",
       "      <td>0.577734</td>\n",
       "      <td>0.894592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.236368</td>\n",
       "      <td>0.921097</td>\n",
       "      <td>0.722905</td>\n",
       "      <td>0.609572</td>\n",
       "      <td>0.888004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for fold 4 to results_with_lstm\\fold_4\n",
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassificationWithLSTM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'fgpos_embedding.weight', 'lstm.bias_hh_l0', 'lstm.bias_hh_l0_reverse', 'lstm.bias_ih_l0', 'lstm.bias_ih_l0_reverse', 'lstm.weight_hh_l0', 'lstm.weight_hh_l0_reverse', 'lstm.weight_ih_l0', 'lstm.weight_ih_l0_reverse', 'pos_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1638' max='1638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1638/1638 09:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.262273</td>\n",
       "      <td>0.893728</td>\n",
       "      <td>0.666537</td>\n",
       "      <td>0.533727</td>\n",
       "      <td>0.887339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.920048</td>\n",
       "      <td>0.724854</td>\n",
       "      <td>0.616290</td>\n",
       "      <td>0.879845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.263100</td>\n",
       "      <td>0.926976</td>\n",
       "      <td>0.738856</td>\n",
       "      <td>0.645910</td>\n",
       "      <td>0.863049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for fold 5 to results_with_lstm\\fold_5\n",
      "\n",
      "Cross-validated results over 5 folds (with all features):\n",
      "F1: 0.7273 ± 0.0063\n",
      "Precision: 0.6260\n",
      "Recall: 0.8683\n",
      "Validation loss (mean): 0.2503\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(processed_train_data)):\n",
    "    print(f\"\\n=== Fold {fold_idx + 1}/{K} ===\")\n",
    "    \n",
    "    train_split = [processed_train_data[i] for i in train_idx]\n",
    "    val_split = [processed_train_data[i] for i in val_idx]\n",
    "\n",
    "    train_dataset_fold = MetaphorDatasetWithAllFeatures(train_split, pos2id, fgpos2id)\n",
    "    val_dataset_fold = MetaphorDatasetWithAllFeatures(val_split, pos2id, fgpos2id)\n",
    "\n",
    "    class_weights, _, _ = get_class_weights(train_dataset_fold)\n",
    "\n",
    "    # Instantiate the custom model for each fold\n",
    "    model = RobertaForTokenClassificationWithLSTM.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        pos_vocab_size=pos_vocab_size,\n",
    "        fgpos_vocab_size=fgpos_vocab_size,\n",
    "        pos_embedding_dim=50,\n",
    "        fgpos_embedding_dim=50,\n",
    "        lstm_hidden_size=128\n",
    "    )\n",
    "\n",
    "    idx_folder = os.path.join('results_with_lstm', f'fold_{fold_idx + 1}')\n",
    "    os.makedirs(idx_folder, exist_ok=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=idx_folder,\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16, # May need to reduce if memory is an issue\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=50,\n",
    "        seed=42 + fold_idx,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_fold,\n",
    "        eval_dataset=val_dataset_fold,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    fold_f1s.append(metrics[\"eval_f1\"])\n",
    "    fold_precisions.append(metrics[\"eval_precision\"])\n",
    "    fold_recalls.append(metrics[\"eval_recall\"])\n",
    "    fold_losses.append(metrics[\"eval_loss\"])\n",
    "\n",
    "    trainer.save_model(idx_folder)\n",
    "    print(f\"Saved model for fold {fold_idx + 1} to {idx_folder}\")\n",
    "\n",
    "# Aggregate results\n",
    "mean_f1 = np.mean(fold_f1s)\n",
    "std_f1 = np.std(fold_f1s)\n",
    "mean_precision = np.mean(fold_precisions)\n",
    "mean_recall = np.mean(fold_recalls)\n",
    "mean_loss = np.mean(fold_losses)\n",
    "\n",
    "print(f\"\\nCross-validated results over {K} folds (with all features):\")\n",
    "print(f\"F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {mean_precision:.4f}\")\n",
    "print(f\"Recall: {mean_recall:.4f}\")\n",
    "print(f\"Validation loss (mean): {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcb6c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 models for ensemble prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Ensemble Performance by Adjusting Vote Count ---\n",
      "Required Votes | Precision | Recall    | F1-Score  | Accuracy\n",
      "---------------------------------------------------------------\n",
      "3 of 5      | 0.5352    | 0.7361    | 0.6198    | 0.8380   \n",
      "4 of 5      | 0.5615    | 0.6806    | 0.6153    | 0.8474   \n",
      "5 of 5      | 0.6004    | 0.6015    | 0.6009    | 0.8567   \n"
     ]
    }
   ],
   "source": [
    "# --- ENSEMBLE EVALUATION ---\n",
    "\n",
    "# Load all fold models\n",
    "model_dirs = sorted(glob.glob(os.path.join(\"results_with_lstm\", \"fold_*\")))\n",
    "models = []\n",
    "for d in model_dirs:\n",
    "    if os.path.exists(os.path.join(d, \"pytorch_model.bin\")) or os.path.exists(os.path.join(d, \"model.safetensors\")):\n",
    "        model = RobertaForTokenClassificationWithLSTM.from_pretrained(\n",
    "            d,\n",
    "            pos_vocab_size=pos_vocab_size,\n",
    "            fgpos_vocab_size=fgpos_vocab_size\n",
    "        )\n",
    "        models.append(model)\n",
    "    else:\n",
    "        print(f\"Warning: Model not found in {d}, skipping.\")\n",
    "\n",
    "print(f\"Loaded {len(models)} models for ensemble prediction.\")\n",
    "\n",
    "# Create a dummy trainer for prediction\n",
    "if models:\n",
    "    args = TrainingArguments(output_dir=\"./inference_tmp_lstm\", per_device_eval_batch_size=8)\n",
    "    predictor = Trainer(model=models[0], args=args, data_collator=data_collator)\n",
    "\n",
    "# Get predictions\n",
    "per_model_logits = []\n",
    "for model in models:\n",
    "    predictor.model = model.to(predictor.args.device)\n",
    "    pred_out = predictor.predict(test_dataset)\n",
    "    per_model_logits.append(pred_out.predictions)\n",
    "\n",
    "per_model_logits = np.stack(per_model_logits, axis=0)\n",
    "\n",
    "# --- Analysis by Adjusting Majority Vote Threshold ---\n",
    "n_models = per_model_logits.shape[0]\n",
    "per_model_preds = np.argmax(per_model_logits, axis=-1)\n",
    "\n",
    "labels = np.stack([item['labels'].numpy() for item in test_dataset])\n",
    "mask = labels != -100\n",
    "y_true = labels[mask]\n",
    "\n",
    "print(\"\\n--- Evaluating Ensemble Performance by Adjusting Vote Count ---\")\n",
    "print(f\"Required Votes | Precision | Recall    | F1-Score  | Accuracy\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "for required_votes in range(int(n_models / 2) + 1, n_models + 1):\n",
    "    vote_sum = per_model_preds.sum(axis=0)\n",
    "    y_pred_at_threshold = (vote_sum[mask] >= required_votes).astype(int)\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_at_threshold, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(y_true, y_pred_at_threshold)\n",
    "\n",
    "    print(f\"{required_votes} of {n_models}      | {prec:<9.4f} | {rec:<9.4f} | {f1:<9.4f} | {acc:<9.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
