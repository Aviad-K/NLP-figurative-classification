{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7eb7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification, # Added this import\n",
    "    TrainingArguments,             # Added this import\n",
    "    Trainer,                        # Added this import\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed2e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"vua_dataset\"\n",
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851a3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK punkt tokenizer data if you haven't already\n",
    "# This block should be executed successfully before nltk.word_tokenize is used.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError: # Catching LookupError as it's the specific error for resource not found\n",
    "    print(\"NLTK 'punkt' tokenizer data not found. Downloading...\")\n",
    "    nltk.download('punkt', quiet=True) # Use quiet=True to suppress progress bar if preferred\n",
    "    print(\"NLTK 'punkt' tokenizer data downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during NLTK data check/download: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f67ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aviad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce3998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name is something like \"roberta-base\"\n",
    "if \"roberta\" in model_name.lower():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,  # required for pre-tokenized input with RoBERTa\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8f6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(json_path, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Loads raw data from a JSONL file, groups it by sentence,\n",
    "    and processes it into a format suitable for MetaphorDataset.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): The path to the JSONL data file.\n",
    "        dataset_name (str): A name for the dataset (e.g., \"TRAIN\", \"TEST\") for logging.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              \"sentence_words\" (list of str) and \"labels\" (list of int).\n",
    "    \"\"\"\n",
    "    data_raw = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data_raw.append(json.loads(line))\n",
    "\n",
    "    sentence_groups = defaultdict(list)\n",
    "    for entry in data_raw:\n",
    "        sentence_groups[entry[\"sentence\"]].append(entry)\n",
    "\n",
    "    processed_data = []\n",
    "    for sentence, entries in sentence_groups.items():\n",
    "        entries = sorted(entries, key=lambda x: x[\"w_index\"])\n",
    "        words_from_sentence = nltk.word_tokenize(sentence)\n",
    "        current_labels = [0] * len(words_from_sentence)\n",
    "\n",
    "        w_index_mismatch_found = False\n",
    "        for entry in entries:\n",
    "            word_index = entry[\"w_index\"]\n",
    "            label_value = entry[\"label\"]\n",
    "            if 0 <= word_index < len(words_from_sentence):\n",
    "                current_labels[word_index] = label_value\n",
    "            else:\n",
    "                w_index_mismatch_found = True\n",
    "                print(f\"CRITICAL WARNING ({dataset_name}): w_index {word_index} out of bounds for NLTK tokenized sentence (length {len(words_from_sentence)}): '{sentence}'\")\n",
    "                print(f\"NLTK Tokens: {words_from_sentence}\")\n",
    "        if w_index_mismatch_found:\n",
    "            print(f\"Skipping problematic {dataset_name} sentence due to w_index mismatch: '{sentence}'\")\n",
    "            continue\n",
    "        processed_data.append({\"sentence_words\": words_from_sentence, \"labels\": current_labels})\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052636ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        sentence_words = entry[\"sentence_words\"] # This is now the list of words\n",
    "        word_labels = entry[\"labels\"]  # list of 0/1 for each original word\n",
    "\n",
    "        # Tokenize the input. is_split_into_words=True is crucial here\n",
    "        # Temporarily omit return_tensors=\"pt\" to get the BatchEncoding object first\n",
    "        raw_encoding = tokenizer(\n",
    "            sentence_words,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            is_split_into_words=True, # Tells tokenizer input is already word-split\n",
    "            # We will convert to tensors after getting word_ids\n",
    "        )\n",
    "\n",
    "        # Get word IDs from the raw_encoding object\n",
    "        word_ids = raw_encoding.word_ids(batch_index=0) # batch_index=0 since we're processing one example at a time\n",
    "\n",
    "        # Now, align the word_labels to the subword tokens\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (CLS, SEP, PAD) or subword tokens that are not\n",
    "            # the first part of a word are ignored (-100)\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            # Only label the first subword token of a given original word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(word_labels[word_idx])\n",
    "            # Subsequent subword tokens of the same word are ignored\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # Convert raw_encoding to tensors and add labels\n",
    "        # This is where return_tensors=\"pt\" functionality is applied\n",
    "        encoding = {k: torch.tensor(v).squeeze(0) for k, v in raw_encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Ensure the labels list has the same length as input_ids\n",
    "        assert len(labels) == len(encoding[\"input_ids\"]), \"Labels and input_ids length mismatch!\"\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec14c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10909\n"
     ]
    }
   ],
   "source": [
    "# --- Load and process TRAIN data using the function ---\n",
    "train_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_train.json\")\n",
    "processed_train_data = load_and_process_data(train_json_path, dataset_name=\"TRAIN\")\n",
    "\n",
    "train_dataset = MetaphorDataset(processed_train_data)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31887931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 3601\n"
     ]
    }
   ],
   "source": [
    "# --- Load and process TEST data using the function ---\n",
    "test_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_test.json\")\n",
    "processed_test_data = load_and_process_data(test_json_path, dataset_name=\"TEST\")\n",
    "test_dataset = MetaphorDataset(processed_test_data)\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81849d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define compute_metrics function ---\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # predictions are logits, take argmax to get predicted class\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (where label is -100)\n",
    "    # Flatten the arrays to work with scikit-learn metrics\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for p_val, l_val in zip(prediction, label):\n",
    "            if l_val != -100:\n",
    "                true_labels.append(l_val)\n",
    "                predicted_labels.append(p_val)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    # Calculate precision, recall, f1-score\n",
    "    # 'binary' for 2 classes (0 and 1)\n",
    "    # 'pos_label=1' means we focus on class 1 (figurative/metaphorical) as the positive class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predicted_labels, average='binary', pos_label=1\n",
    "    )\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e340489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2 # Assuming 0 for literal, 1 for figurative\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",       # Save model checkpoint at the end of each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    # Add these for clearer metrics during evaluation\n",
    "    load_best_model_at_end=True, # Load the best model found during training based on eval_metric\n",
    "    metric_for_best_model=\"eval_f1\", # Or \"eval_accuracy\", \"eval_f1\" if you define compute_metrics\n",
    "    greater_is_better=True, # For loss, lower is better\n",
    "    \n",
    "    learning_rate=2e-5,                # try a slightly smaller LR\n",
    "    weight_decay=0.01,                 # regularization\n",
    "    warmup_ratio=0.1,                  # linear warmup for first 10%\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbc0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset, # Now correctly defined\n",
    "#     # You might want to add a data_collator and compute_metrics here later\n",
    "#     # data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8826bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(train_dataset):\n",
    "    labels_flat = np.concatenate([x['labels'] for x in train_dataset])\n",
    "    labels_filtered = labels_flat[labels_flat != -100]\n",
    "    counts = Counter(labels_filtered)\n",
    "    total = sum(counts.values())\n",
    "    return torch.tensor(\n",
    "        [total / counts[0], total / counts[1]], dtype=torch.float\n",
    "    ), counts, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08734442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights, counts, _ = get_class_weights(train_dataset)\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        active_loss = labels.view(-1) != -100\n",
    "        active_logits = logits.view(-1, model.config.num_labels)[active_loss]\n",
    "        active_labels = labels.view(-1)[active_loss]\n",
    "\n",
    "        weights = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "        loss_fct = CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f382ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9240ec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: Counter({0: 159865, 1: 19122})\n",
      "Class weights: tensor([1.1196, 9.3603])\n"
     ]
    }
   ],
   "source": [
    "print(\"Label counts:\", counts)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a7bf4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='513' max='513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [513/513 17:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.349888</td>\n",
       "      <td>0.844719</td>\n",
       "      <td>0.429014</td>\n",
       "      <td>0.288502</td>\n",
       "      <td>0.836350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.358403</td>\n",
       "      <td>0.850698</td>\n",
       "      <td>0.449758</td>\n",
       "      <td>0.302688</td>\n",
       "      <td>0.874811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.352108</td>\n",
       "      <td>0.863691</td>\n",
       "      <td>0.464895</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>0.848919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=513, training_loss=0.29431310174060843, metrics={'train_runtime': 1036.5461, 'train_samples_per_second': 31.573, 'train_steps_per_second': 0.495, 'total_flos': 2137864739424768.0, 'train_loss': 0.29431310174060843, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b448fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='901' max='901' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [901/901 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3521082103252411,\n",
       " 'eval_accuracy': 0.8636905596857904,\n",
       " 'eval_f1': 0.4648953744493392,\n",
       " 'eval_precision': 0.3200947867298578,\n",
       " 'eval_recall': 0.8489190548014077,\n",
       " 'eval_runtime': 39.0851,\n",
       " 'eval_samples_per_second': 92.132,\n",
       " 'eval_steps_per_second': 23.052,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee154ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and process TRAIN data using the function ---\n",
    "train_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_train.json\")\n",
    "processed_train_data = load_and_process_data(train_json_path, dataset_name=\"TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "334ff31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9818\n",
      "Number of validation samples: 1091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data_split, val_data_split = train_test_split(\n",
    "    processed_train_data,\n",
    "    test_size=0.1, # Using 10% of the training data for validation\n",
    "    random_state=42 # for reproducibility\n",
    ")\n",
    "\n",
    "train_dataset = MetaphorDataset(train_data_split)\n",
    "val_dataset = MetaphorDataset(val_data_split)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b75606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # number of folds\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "fold_f1s = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_losses = []\n",
    "trainers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a73d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='819' max='819' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [819/819 13:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.272813</td>\n",
       "      <td>0.857669</td>\n",
       "      <td>0.583240</td>\n",
       "      <td>0.426638</td>\n",
       "      <td>0.921474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.240904</td>\n",
       "      <td>0.879752</td>\n",
       "      <td>0.623365</td>\n",
       "      <td>0.471193</td>\n",
       "      <td>0.920712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.252221</td>\n",
       "      <td>0.916090</td>\n",
       "      <td>0.689501</td>\n",
       "      <td>0.574526</td>\n",
       "      <td>0.862008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='275' max='819' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [275/819 21:39 < 43:09, 0.21 it/s, Epoch 1.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.246109</td>\n",
       "      <td>0.888232</td>\n",
       "      <td>0.633450</td>\n",
       "      <td>0.488237</td>\n",
       "      <td>0.901610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\HW\\Current HW\\current HW\\NLP\\final project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(processed_train_data)):\n",
    "    print(f\"\\n=== Fold {fold_idx + 1}/{K} ===\")\n",
    "    # Split raw data (assuming processed_train_data is indexable list-like)\n",
    "    idx_folder = os.path.join('results', f'fold_{fold_idx + 1}')\n",
    "    os.makedirs(idx_folder, exist_ok=True)\n",
    "    train_split = [processed_train_data[i] for i in train_idx]\n",
    "    val_split = [processed_train_data[i] for i in val_idx]\n",
    "\n",
    "    # Build datasets (your existing dataset wrapper handles tokenization/alignment inside)\n",
    "    train_dataset = MetaphorDataset(train_split)\n",
    "    val_dataset = MetaphorDataset(val_split)\n",
    "\n",
    "    # Recompute class weights from this fold's train data\n",
    "    class_weights, _, _ = get_class_weights(train_dataset)\n",
    "\n",
    "    # Fresh model per fold\n",
    "    if \"roberta\" in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_prefix_space=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=model.config.num_labels if hasattr(model, \"config\") else model.config.num_labels if False else None)\n",
    "    # (Above line may need adjustment to your existing instantiation logic; ensure num_labels is correct)\n",
    "\n",
    "    # Training arguments: you can customize per fold output_dir to avoid overwrite\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=idx_folder,\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "        seed=42 + fold_idx,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    fold_f1s.append(metrics[\"eval_f1\"])\n",
    "    fold_precisions.append(metrics[\"eval_precision\"])\n",
    "    fold_recalls.append(metrics[\"eval_recall\"])\n",
    "    fold_losses.append(metrics[\"eval_loss\"])\n",
    "\n",
    "    trainers.append(trainer)\n",
    "\n",
    "# Aggregate results\n",
    "mean_f1 = np.mean(fold_f1s)\n",
    "std_f1 = np.std(fold_f1s)\n",
    "mean_precision = np.mean(fold_precisions)\n",
    "mean_recall = np.mean(fold_recalls)\n",
    "mean_loss = np.mean(fold_losses)\n",
    "\n",
    "print(f\"\\nCross-validated results over {K} folds:\")\n",
    "print(f\"F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {mean_precision:.4f}\")\n",
    "print(f\"Recall: {mean_recall:.4f}\")\n",
    "print(f\"Validation loss (mean): {mean_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
