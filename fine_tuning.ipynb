{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de7eb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification, # Added this import\n",
    "    TrainingArguments,             # Added this import\n",
    "    Trainer,                        # Added this import\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed2e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"vua_dataset\"\n",
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851a3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK punkt tokenizer data if you haven't already\n",
    "# This block should be executed successfully before nltk.word_tokenize is used.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError: # Catching LookupError as it's the specific error for resource not found\n",
    "    print(\"NLTK 'punkt' tokenizer data not found. Downloading...\")\n",
    "    nltk.download('punkt', quiet=True) # Use quiet=True to suppress progress bar if preferred\n",
    "    print(\"NLTK 'punkt' tokenizer data downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during NLTK data check/download: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f67ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aviad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce3998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_name is something like \"roberta-base\"\n",
    "if \"roberta\" in model_name.lower():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,  # required for pre-tokenized input with RoBERTa\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8f6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(json_path, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Loads raw data from a JSONL file, groups it by sentence,\n",
    "    and processes it into a format suitable for MetaphorDataset.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): The path to the JSONL data file.\n",
    "        dataset_name (str): A name for the dataset (e.g., \"TRAIN\", \"TEST\") for logging.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains\n",
    "              \"sentence_words\" (list of str) and \"labels\" (list of int).\n",
    "    \"\"\"\n",
    "    data_raw = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data_raw.append(json.loads(line))\n",
    "\n",
    "    sentence_groups = defaultdict(list)\n",
    "    for entry in data_raw:\n",
    "        sentence_groups[entry[\"sentence\"]].append(entry)\n",
    "\n",
    "    processed_data = []\n",
    "    for sentence, entries in sentence_groups.items():\n",
    "        entries = sorted(entries, key=lambda x: x[\"w_index\"])\n",
    "        words_from_sentence = nltk.word_tokenize(sentence)\n",
    "        current_labels = [0] * len(words_from_sentence)\n",
    "\n",
    "        w_index_mismatch_found = False\n",
    "        for entry in entries:\n",
    "            word_index = entry[\"w_index\"]\n",
    "            label_value = entry[\"label\"]\n",
    "            if 0 <= word_index < len(words_from_sentence):\n",
    "                current_labels[word_index] = label_value\n",
    "            else:\n",
    "                w_index_mismatch_found = True\n",
    "                print(f\"CRITICAL WARNING ({dataset_name}): w_index {word_index} out of bounds for NLTK tokenized sentence (length {len(words_from_sentence)}): '{sentence}'\")\n",
    "                print(f\"NLTK Tokens: {words_from_sentence}\")\n",
    "        if w_index_mismatch_found:\n",
    "            print(f\"Skipping problematic {dataset_name} sentence due to w_index mismatch: '{sentence}'\")\n",
    "            continue\n",
    "        processed_data.append({\"sentence_words\": words_from_sentence, \"labels\": current_labels})\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052636ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaphorDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        sentence_words = entry[\"sentence_words\"] # This is now the list of words\n",
    "        word_labels = entry[\"labels\"]  # list of 0/1 for each original word\n",
    "\n",
    "        # Tokenize the input. is_split_into_words=True is crucial here\n",
    "        # Temporarily omit return_tensors=\"pt\" to get the BatchEncoding object first\n",
    "        raw_encoding = tokenizer(\n",
    "            sentence_words,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            is_split_into_words=True, # Tells tokenizer input is already word-split\n",
    "            # We will convert to tensors after getting word_ids\n",
    "        )\n",
    "\n",
    "        # Get word IDs from the raw_encoding object\n",
    "        word_ids = raw_encoding.word_ids(batch_index=0) # batch_index=0 since we're processing one example at a time\n",
    "\n",
    "        # Now, align the word_labels to the subword tokens\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (CLS, SEP, PAD) or subword tokens that are not\n",
    "            # the first part of a word are ignored (-100)\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            # Only label the first subword token of a given original word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(word_labels[word_idx])\n",
    "            # Subsequent subword tokens of the same word are ignored\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # Convert raw_encoding to tensors and add labels\n",
    "        # This is where return_tensors=\"pt\" functionality is applied\n",
    "        encoding = {k: torch.tensor(v).squeeze(0) for k, v in raw_encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Ensure the labels list has the same length as input_ids\n",
    "        assert len(labels) == len(encoding[\"input_ids\"]), \"Labels and input_ids length mismatch!\"\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec14c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10909\n"
     ]
    }
   ],
   "source": [
    "# --- Load and process TRAIN data using the function ---\n",
    "train_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_train.json\")\n",
    "processed_train_data = load_and_process_data(train_json_path, dataset_name=\"TRAIN\")\n",
    "\n",
    "train_dataset = MetaphorDataset(processed_train_data)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31887931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 3601\n"
     ]
    }
   ],
   "source": [
    "# --- Load and process TEST data using the function ---\n",
    "test_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_test.json\")\n",
    "processed_test_data = load_and_process_data(test_json_path, dataset_name=\"TEST\")\n",
    "test_dataset = MetaphorDataset(processed_test_data)\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81849d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define compute_metrics function ---\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # predictions are logits, take argmax to get predicted class\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (where label is -100)\n",
    "    # Flatten the arrays to work with scikit-learn metrics\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for p_val, l_val in zip(prediction, label):\n",
    "            if l_val != -100:\n",
    "                true_labels.append(l_val)\n",
    "                predicted_labels.append(p_val)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    # Calculate precision, recall, f1-score\n",
    "    # 'binary' for 2 classes (0 and 1)\n",
    "    # 'pos_label=1' means we focus on class 1 (figurative/metaphorical) as the positive class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predicted_labels, average='binary', pos_label=1\n",
    "    )\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e340489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2 # Assuming 0 for literal, 1 for figurative\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",       # Save model checkpoint at the end of each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    # Add these for clearer metrics during evaluation\n",
    "    load_best_model_at_end=True, # Load the best model found during training based on eval_metric\n",
    "    metric_for_best_model=\"eval_f1\", # Or \"eval_accuracy\", \"eval_f1\" if you define compute_metrics\n",
    "    greater_is_better=True, # For loss, lower is better\n",
    "    \n",
    "    learning_rate=2e-5,                # try a slightly smaller LR\n",
    "    weight_decay=0.01,                 # regularization\n",
    "    warmup_ratio=0.1,                  # linear warmup for first 10%\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbc0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset, # Now correctly defined\n",
    "#     # You might want to add a data_collator and compute_metrics here later\n",
    "#     # data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8826bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(train_dataset):\n",
    "    labels_flat = np.concatenate([x['labels'] for x in train_dataset])\n",
    "    labels_filtered = labels_flat[labels_flat != -100]\n",
    "    counts = Counter(labels_filtered)\n",
    "    total = sum(counts.values())\n",
    "    return torch.tensor(\n",
    "        [total / counts[0], total / counts[1]], dtype=torch.float\n",
    "    ), counts, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08734442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights, counts, _ = get_class_weights(train_dataset)\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        active_loss = labels.view(-1) != -100\n",
    "        active_logits = logits.view(-1, model.config.num_labels)[active_loss]\n",
    "        active_labels = labels.view(-1)[active_loss]\n",
    "\n",
    "        weights = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "        loss_fct = CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f382ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9240ec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: Counter({0: 159865, 1: 19122})\n",
      "Class weights: tensor([1.1196, 9.3603])\n"
     ]
    }
   ],
   "source": [
    "print(\"Label counts:\", counts)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a7bf4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='513' max='513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [513/513 08:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.343671</td>\n",
       "      <td>0.855976</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.301574</td>\n",
       "      <td>0.809201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238600</td>\n",
       "      <td>0.347952</td>\n",
       "      <td>0.860254</td>\n",
       "      <td>0.457676</td>\n",
       "      <td>0.313771</td>\n",
       "      <td>0.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.355157</td>\n",
       "      <td>0.864515</td>\n",
       "      <td>0.466257</td>\n",
       "      <td>0.321459</td>\n",
       "      <td>0.848416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=513, training_loss=0.29126956774244994, metrics={'train_runtime': 516.3322, 'train_samples_per_second': 63.384, 'train_steps_per_second': 0.994, 'total_flos': 2137864739424768.0, 'train_loss': 0.29126956774244994, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b448fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='901' max='901' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [901/901 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35515719652175903,\n",
       " 'eval_accuracy': 0.8645146584373685,\n",
       " 'eval_f1': 0.46625682116460593,\n",
       " 'eval_precision': 0.321459186589199,\n",
       " 'eval_recall': 0.8484162895927602,\n",
       " 'eval_runtime': 23.2949,\n",
       " 'eval_samples_per_second': 154.583,\n",
       " 'eval_steps_per_second': 38.678,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee154ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and process TRAIN data using the function ---\n",
    "train_json_path = os.path.join(\"vua_dataset\", \"vua20_metaphor_train.json\")\n",
    "processed_train_data = load_and_process_data(train_json_path, dataset_name=\"TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "334ff31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9818\n",
      "Number of validation samples: 1091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data_split, val_data_split = train_test_split(\n",
    "    processed_train_data,\n",
    "    test_size=0.1, # Using 10% of the training data for validation\n",
    "    random_state=42 # for reproducibility\n",
    ")\n",
    "\n",
    "train_dataset = MetaphorDataset(train_data_split)\n",
    "val_dataset = MetaphorDataset(val_data_split)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # number of folds\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "fold_f1s = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a73d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [411/411 07:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.283828</td>\n",
       "      <td>0.852258</td>\n",
       "      <td>0.572926</td>\n",
       "      <td>0.416628</td>\n",
       "      <td>0.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.251078</td>\n",
       "      <td>0.894721</td>\n",
       "      <td>0.644269</td>\n",
       "      <td>0.507456</td>\n",
       "      <td>0.882084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.247528</td>\n",
       "      <td>0.903318</td>\n",
       "      <td>0.662835</td>\n",
       "      <td>0.531899</td>\n",
       "      <td>0.879288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [411/411 06:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.261744</td>\n",
       "      <td>0.864882</td>\n",
       "      <td>0.592336</td>\n",
       "      <td>0.437584</td>\n",
       "      <td>0.916432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.234544</td>\n",
       "      <td>0.890832</td>\n",
       "      <td>0.641946</td>\n",
       "      <td>0.494810</td>\n",
       "      <td>0.913621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.226660</td>\n",
       "      <td>0.903315</td>\n",
       "      <td>0.665973</td>\n",
       "      <td>0.528599</td>\n",
       "      <td>0.899821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [411/411 08:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.363800</td>\n",
       "      <td>0.270139</td>\n",
       "      <td>0.888047</td>\n",
       "      <td>0.630025</td>\n",
       "      <td>0.491601</td>\n",
       "      <td>0.876956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.239525</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>0.660565</td>\n",
       "      <td>0.525196</td>\n",
       "      <td>0.889950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.238742</td>\n",
       "      <td>0.908166</td>\n",
       "      <td>0.675560</td>\n",
       "      <td>0.548355</td>\n",
       "      <td>0.879608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [411/411 17:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.266568</td>\n",
       "      <td>0.883512</td>\n",
       "      <td>0.611286</td>\n",
       "      <td>0.467404</td>\n",
       "      <td>0.883145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.245114</td>\n",
       "      <td>0.892808</td>\n",
       "      <td>0.636777</td>\n",
       "      <td>0.490912</td>\n",
       "      <td>0.905966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.240968</td>\n",
       "      <td>0.906867</td>\n",
       "      <td>0.663507</td>\n",
       "      <td>0.530565</td>\n",
       "      <td>0.885345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 01:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [411/411 14:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356800</td>\n",
       "      <td>0.266061</td>\n",
       "      <td>0.886819</td>\n",
       "      <td>0.623936</td>\n",
       "      <td>0.482920</td>\n",
       "      <td>0.881273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.236949</td>\n",
       "      <td>0.888969</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.488717</td>\n",
       "      <td>0.913088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.235270</td>\n",
       "      <td>0.907488</td>\n",
       "      <td>0.671815</td>\n",
       "      <td>0.539997</td>\n",
       "      <td>0.888774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 01:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validated results over 5 folds:\n",
      "F1: 0.6679 ± 0.0050\n",
      "Precision: 0.5359\n",
      "Recall: 0.8866\n",
      "Validation loss (mean): 0.2378\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(processed_train_data)):\n",
    "    print(f\"\\n=== Fold {fold_idx + 1}/{K} ===\")\n",
    "    # Split raw data (assuming processed_train_data is indexable list-like)\n",
    "    idx_folder = os.path.join('results', f'fold_{fold_idx + 1}')\n",
    "    os.makedirs(idx_folder, exist_ok=True)\n",
    "    train_split = [processed_train_data[i] for i in train_idx]\n",
    "    val_split = [processed_train_data[i] for i in val_idx]\n",
    "\n",
    "    # Build datasets (your existing dataset wrapper handles tokenization/alignment inside)\n",
    "    train_dataset = MetaphorDataset(train_split)\n",
    "    val_dataset = MetaphorDataset(val_split)\n",
    "\n",
    "    # Recompute class weights from this fold's train data\n",
    "    class_weights, _, _ = get_class_weights(train_dataset)\n",
    "\n",
    "    # Fresh model per fold\n",
    "    if \"roberta\" in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_prefix_space=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=model.config.num_labels if hasattr(model, \"config\") else model.config.num_labels if False else None)\n",
    "    # (Above line may need adjustment to your existing instantiation logic; ensure num_labels is correct)\n",
    "\n",
    "    # Training arguments: you can customize per fold output_dir to avoid overwrite\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=idx_folder,\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "        seed=42 + fold_idx,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    fold_f1s.append(metrics[\"eval_f1\"])\n",
    "    fold_precisions.append(metrics[\"eval_precision\"])\n",
    "    fold_recalls.append(metrics[\"eval_recall\"])\n",
    "    fold_losses.append(metrics[\"eval_loss\"])\n",
    "\n",
    "    trainer.save_model(idx_folder)\n",
    "    print(f\"Saved model for fold {fold_idx + 1} to {idx_folder}\")\n",
    "\n",
    "# Aggregate results\n",
    "mean_f1 = np.mean(fold_f1s)\n",
    "std_f1 = np.std(fold_f1s)\n",
    "mean_precision = np.mean(fold_precisions)\n",
    "mean_recall = np.mean(fold_recalls)\n",
    "mean_loss = np.mean(fold_losses)\n",
    "\n",
    "print(f\"\\nCross-validated results over {K} folds:\")\n",
    "print(f\"F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Precision: {mean_precision:.4f}\")\n",
    "print(f\"Recall: {mean_recall:.4f}\")\n",
    "print(f\"Validation loss (mean): {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "174a630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 trainers.\n"
     ]
    }
   ],
   "source": [
    "# Minimal loader: load all fold models into `trainers`\n",
    "model_dirs = sorted(glob.glob(os.path.join(\"results\", \"fold_*\")))\n",
    "\n",
    "trainers = []\n",
    "for d in model_dirs:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(d)\n",
    "    args = TrainingArguments(output_dir=os.path.join(d, \"inference_tmp\"), per_device_eval_batch_size=8)\n",
    "    tr = Trainer(model=model, args=args, data_collator=data_collator)\n",
    "    trainers.append(tr)\n",
    "\n",
    "print(f\"Loaded {len(trainers)} trainers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc0ef3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aviad\\Desktop\\NLP-figurative-classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Ensemble Performance at Different Thresholds ---\n",
      "Threshold | Precision | Recall    | F1-Score  | Accuracy\n",
      "----------------------------------------------------------\n",
      "0.50      | 0.3248    | 0.8457    | 0.4693    | 0.8666   \n",
      "0.55      | 0.3341    | 0.8243    | 0.4755    | 0.8731   \n",
      "0.60      | 0.3429    | 0.8009    | 0.4802    | 0.8791   \n",
      "0.65      | 0.3521    | 0.7735    | 0.4840    | 0.8849   \n",
      "0.70      | 0.3615    | 0.7443    | 0.4866    | 0.8905   \n",
      "0.75      | 0.3694    | 0.7064    | 0.4851    | 0.8954   \n",
      "0.80      | 0.3761    | 0.6599    | 0.4791    | 0.8999   \n",
      "0.85      | 0.3830    | 0.6006    | 0.4677    | 0.9047   \n",
      "0.90      | 0.3800    | 0.5055    | 0.4339    | 0.9080   \n",
      "0.95      | 0.3562    | 0.3477    | 0.3519    | 0.9107   \n"
     ]
    }
   ],
   "source": [
    "# --- Analysis of Classification Thresholds ---\n",
    "\n",
    "# 1) Get raw logits from each model in the ensemble\n",
    "per_model_logits = []\n",
    "if not trainers:\n",
    "    print(\"Trainers list is empty. Please run the model loading cell first.\")\n",
    "else:\n",
    "    for t in trainers:\n",
    "        pred_out = t.predict(test_dataset)\n",
    "        per_model_logits.append(pred_out.predictions)\n",
    "\n",
    "    per_model_logits = np.stack(per_model_logits, axis=0)  # [n_models, n_samples, seq_len, n_labels]\n",
    "\n",
    "    # 2) Convert logits to probabilities and average them\n",
    "    per_model_probs = softmax(per_model_logits, axis=-1)  # Softmax over the label dimension\n",
    "    ensemble_probs = per_model_probs.mean(axis=0)  # Average probs across models -> [n_samples, seq_len, n_labels]\n",
    "\n",
    "    # 3) Get the probabilities for the \"metaphor\" class (class 1)\n",
    "    ensemble_probs_class1 = ensemble_probs[..., 1]  # Shape: [n_samples, seq_len]\n",
    "\n",
    "    # 4) Get the ground truth labels and the mask to ignore -100 values\n",
    "    labels = np.stack([\n",
    "        (test_dataset[i][\"labels\"].numpy() if hasattr(test_dataset[i][\"labels\"], \"numpy\") else np.array(test_dataset[i][\"labels\"]))\n",
    "        for i in range(len(test_dataset))\n",
    "    ])\n",
    "    mask = labels != -100\n",
    "    y_true = labels[mask]\n",
    "\n",
    "    # 5) Iterate through different thresholds and evaluate metrics\n",
    "    print(\"--- Evaluating Ensemble Performance at Different Thresholds ---\")\n",
    "    print(\"Threshold | Precision | Recall    | F1-Score  | Accuracy\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "    for threshold in np.arange(0.5, 1.0, 0.05):\n",
    "        # Apply threshold to the probabilities of the positive class\n",
    "        # Get predictions only for the valid (unmasked) tokens\n",
    "        y_pred_at_threshold = (ensemble_probs_class1[mask] >= threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        # zero_division=0 prevents warnings when a class is not predicted\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred_at_threshold, average=\"binary\", pos_label=1, zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(y_true, y_pred_at_threshold)\n",
    "\n",
    "        print(f\"{threshold:<9.2f} | {prec:<9.4f} | {rec:<9.4f} | {f1:<9.4f} | {acc:<9.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
